# TODO: Write Message to HDFS with Kerberos Authentication

1. Update Project Dependencies
   - [x] Add Hadoop Common, HDFS, and Auth libraries to the project dependencies (Maven/Gradle).
   - [x] Add Spark Core and Spark SQL dependencies if not already present.
   - [x] Ensure SLF4J/Logback dependencies are present.

2. Load Hadoop and Kerberos Configuration
   - [x] Programmatically load `core-site.xml`, `hdfs-site.xml`, and `hive-site.xml` from `src/main/resources/conf/`.
   - [x] Set the system property `java.security.krb5.conf` to point to `krb5.conf`.

3. Set Up Kerberos Authentication
   - [x] Identify the correct principal for HDFS access (from configs or admin).
   - [x] Use the `hdfs.headless.keytab` for authentication.
   - [x] Use Hadoop's `UserGroupInformation` to log in with the keytab and principal.

4. Configure and Use Spark Context
   - [x] Configure Spark to use the Hadoop and Kerberos settings (pass Hadoop conf directory to Spark).
   - [x] Ensure Spark is able to authenticate with Kerberos and access HDFS.
   - [x] Initialize SparkSession with the correct configuration.
   - [x] Validate Spark can read/write to HDFS as the authenticated user.

5. Write to HDFS via Spark
   - [x] Use Spark (e.g., SparkContext or SparkSession) to write the message to a file in HDFS.
   - [x] Handle exceptions and ensure resources are closed properly.

6. Testing
   - [x] Test the application locally with the provided conf files.
   - [x] Validate that the file is created in HDFS and contains the correct message.
   - [x] Validate that the Spark job runs successfully and can access HDFS with Kerberos.

7. Deployment
   - [x] Package the application with all necessary configuration files.
   - [x] Run the Spark job on a node with network access to the Hadoop cluster.

# Notes
- All configuration files are in `src/main/resources/conf/`.
- Kerberos realm and KDC are defined in `krb5.conf`.
- Keytab file is `hdfs.headless.keytab`.
- HDFS URI and principal should be confirmed with the cluster admin if not obvious from configs.
- Spark must be configured to use the same Hadoop and Kerberos settings for seamless HDFS access. 